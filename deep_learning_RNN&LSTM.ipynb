{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、遞歸神經網路 (Recurrent Neural Network, RNN)\n",
    "\n",
    "<img src=\"img/RNN.JPG\" alt=\"drawing\" width=\"600px\"/>\n",
    "\n",
    "- $\\bf{\\vec{x}}=\\left (x_{0}, x_{1}, x_{2},...,x_{t} \\right )$\n",
    "\n",
    "- $\\bf{\\vec{h}}=\\left ( h_{0}, h_{1}, h_{2},...,h_{t} \\right )$\n",
    "\n",
    "#### 1. 單個 RNN  層(別名: 含狀態層、含記憶層)的運算\n",
    "\n",
    "$$\\bf{h}_{t}=tanh\\left ( \\bf{h}_{t-1}\\bf{W}_{h}+\\bf{x}_{t}\\bf{W}_{x}+ \\bf{b} \\right )$$\n",
    "\n",
    "- $\\bf{h}_{t}$: 隱藏狀態(hidden state)或隱藏狀態向量(hidden state vector)\n",
    "\n",
    "#### 2. 往時間方向展開 RNN  層的反向傳播(Backpropagation Through Time, BPTT)\n",
    "\n",
    "- BPTT 在學習長時間序列資料時的問題:\n",
    "\n",
    "  (1) BPTT 消耗的電腦記憶體用量隨著時間序列資料的大小成正比增加\n",
    "  \n",
    "  (2) 時間愈長，BPTT 的梯度會越不穩定\n",
    "  \n",
    "  \n",
    "- 解決方式: Truncated BPTT\n",
    "\n",
    "  (1) 處理大型時間序列資料時，以適當長度截斷時間軸方向過長的類神經網路，來建立許多小型的類神經網路，之後用這些區塊為單位(TimeRNN)進行反向傳播學習 (維持正向傳播的連結，只切斷反向傳播的連結)\n",
    "\n",
    "  [註] RNN 執行 Truncated BPTT 時，必須依照時間順序提供資料來維持正向傳播時間軸上的連結，不可隨機選擇提供資料\n",
    "\n",
    "\n",
    "<img src=\"img/TimeRNN.JPG\" alt=\"drawing\" width=\"600px\"/>\n",
    "\n",
    "$$\\bf{\\vec{x}_{s}}=\\left (x_{0}, x_{1}, x_{2},...,x_{T-1} \\right ),\\ \\bf{\\vec{h}_{s}}=\\left ( h_{0}, h_{1}, h_{2},...,h_{T-1} \\right )$$\n",
    "\n",
    "\n",
    "- Truncated BPTT 的小批次學習: 按照各個批次，移動資料的起始位置\n",
    "\n",
    "  例子: 1000 個時間序列的輸入資料，以 10 個時間長度為單位進行截斷且小批次的批次數為 2\n",
    "  \n",
    "  -> 第一批次起始位置的元素: $x_{0}$、第二批次起始位置的元素: $x_{500}$\n",
    "  \n",
    "  第一批次: $(x_{0}, x_{1},...,x_{9})$、$(x_{10}, x_{11},...,x_{19})$、...、$(x_{490}, x_{491},...,x_{499})$\n",
    "  \n",
    "  第二批次: $(x_{500}, x_{501},...,x_{509})$、$(x_{510}, x_{511},...,x_{519})$、...、$(x_{990}, x_{991},...,x_{999})$\n",
    "  \n",
    "  矩陣表示:\n",
    "  \n",
    "  $\\begin{bmatrix}\n",
    "  x_{0},\\  x_{1},\\ ...\\ ,\\ x_{9}\\\\ \n",
    "  x_{500},\\ x_{501},...,\\ x_{509}\n",
    "  \\end{bmatrix}$, \n",
    "  $\\begin{bmatrix}\n",
    "  x_{11},\\  x_{12},\\ ...\\ ,\\ x_{19}\\\\ \n",
    "  x_{510},\\ x_{511},...,\\ x_{519}\n",
    "  \\end{bmatrix}$, ... ,\n",
    "  $\\begin{bmatrix}\n",
    "  x_{490},\\ x_{491},...,\\ x_{499}\\\\\n",
    "  x_{990},\\ x_{991},...,\\ x_{999}\n",
    "  \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 建立 RNN 層的類別:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx)\n",
    "        h_next = np.tanh(t)\n",
    "        \n",
    "        self.cache(x, h_prex, h_next)\n",
    "        return h_next\n",
    "    \n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "        \n",
    "        dt = dh_next * (1 - h_next ** 2)\n",
    "        db = np.sum(dt, axis = 0)\n",
    "        dWh = np.dot(h_prev.T, dt)\n",
    "        dh_prev = np.dot(dt, Wh.T)\n",
    "        dWx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "        \n",
    "        # self.grads[0] => 淺拷貝(shallow copy), self.grads[0][...] => 深拷貝(deep copy) \n",
    "        # self.grads[0][...]: 使用三個點[...]固定 Numpy 陣列的記憶體位置並覆寫 Numpy 陣列的元素\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "        \n",
    "        return dx, dh_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 建立 TimeRNN 層的類別:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "    \n",
    "    # stateful = True: 維持 TimeRNN 層的隱藏狀態; stateful = False: 以零矩陣初始化第一個 RNN 層的隱藏狀態\n",
    "    def __init__(self, Wx, Wh, b, stateful = False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "        \n",
    "        self.h, self.dh = None, None\n",
    "        \n",
    "        self.stateful = stateful\n",
    "    \n",
    "    # 設定 TimeRNN 層的的隱藏狀態\n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "        \n",
    "    # 還原 TimeRNN 層的的隱藏狀態\n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype = 'f')\n",
    "        \n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H),dtype = 'f')\n",
    "            \n",
    "        for t in range(T):\n",
    "            layer = RNN(*self.params)\n",
    "            self.h = layer.forward(xs[:, t, :], self.h)\n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "        return hs\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        dhs = np.empty((N, T, D), dtype = 'f')\n",
    "        dh = 0\n",
    "        grads = [0, 0, 0]\n",
    "        \n",
    "        # 往初始時間的方向累加梯度\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[: ,t , :] + dh)\n",
    "            dxs[:, t, :] = dx\n",
    "            \n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "        \n",
    "        # 將梯度覆寫到 self.grads 中\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "            \n",
    "        self.dh = dh\n",
    "        \n",
    "        return dxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、遞歸神經網路(RNN)的問題與因應的對策\n",
    "\n",
    "#### 1. RNN 不擅長學習時間序列的長期依存關係\n",
    "\n",
    "原因: BPTT 可能出現梯度消失或梯度爆炸的問題\n",
    "\n",
    "<img src=\"img/BPTT.JPG\" alt=\"drawing\" width=\"500px\"/>\n",
    "\n",
    "直觀的解釋:\n",
    "\n",
    "- 反向傳播時，tanh 函數微分的數值介於 -1 ~ 1 之間，當通過 tanh 函數 T 次，梯度也會重複減弱 T 次，造成梯度消失\n",
    "\n",
    "  -> 改進方式: 活化函數使用 ReLU 函數\n",
    "  \n",
    "\n",
    "- 若每個 RNN 的權重 $W_{h}$ 皆相同且隱藏狀態變化的傳入值為 dh，則反向傳播通過一系列 Mul 節點後梯度為 $dh\\cdot W_{h}^{T}$\n",
    "\n",
    "  假設 $W_{h}$ 是純量矩陣:\n",
    "  \n",
    "  (1) 當 $W_{h}$ > 1, 梯度指數性增加 => 梯度爆炸(exploding gradients)\n",
    "      \n",
    "  (2) 當 $W_{h}$ < 1, 梯度指數性減少 => 梯度消失(vanishing gradients)\n",
    "  \n",
    "  假設 $W_{h}$ 是非純量矩陣 => 考量矩陣的奇異值是否大於一(必要條件，非充分條件)\n",
    "  \n",
    "#### 2. 處理梯度爆炸的對策: 梯度裁減(gradients clipping)\n",
    "\n",
    "梯度裁減(gradients clipping)演算法: ($\\hat{\\bf{g}}$ 代表所有類神經網路所使用的參數之梯度整合成一個的結果)\n",
    "\n",
    "$$if\\quad || \\hat{\\bf{g}} ||\\geq \\ threshold:$$\n",
    "\n",
    "$$ \\hat{\\bf{g}} = \\frac{threshold}{|| \\hat{\\bf{g}} ||}\\ \\hat{\\bf{g}}$$\n",
    "\n",
    "#### 3. 處理梯度消失的對策: 使用含閘門的 RNN\n",
    "\n",
    "具代表性的結構:\n",
    "\n",
    "- 長短期記憶模型(Long short-term memory, LSTM)\n",
    "\n",
    "\n",
    "- Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三、長短期記憶模型(Long short-term memory, LSTM)\n",
    "\n",
    "#### 1. 單個 LSTM 的計算圖\n",
    "\n",
    "<img src=\"img/LSTMcp.JPG\" alt=\"drawing\" width=\"400px\"/>\n",
    "\n",
    "- 輸入值: $\\bf{x}$\n",
    "\n",
    "\n",
    "- 隱藏狀態(hidden state): $\\bf{h}$\n",
    "\n",
    "\n",
    "- 記憶單元(memory cell): $\\bf{c}$\n",
    "\n",
    "  特色: 僅在 LSTM 層內傳遞資料，不會輸出給其他層\n",
    "\n",
    "[註1] $\\sigma$ 代表 sigmoid 函數\n",
    "\n",
    "[註2] $\\odot$: 阿達馬乘積(hadamard product)，兩個矩陣相同位置的元素相乘\n",
    "\n",
    "**(1) 遺忘閘門(forget gate): f**\n",
    "\n",
    "$$\\bf{f}=\\sigma \\left (\\bf{x_{t}}\\bf{W_{x}^{\\left ( f \\right )}}+ \\bf{h_{t-1}}\\bf{W_{h}^{\\left ( f \\right )}}+b^{\\left ( f \\right )} \\right )$$\n",
    "\n",
    "**說明:** 從上一個時刻的記憶單元中，刪除必須遺忘的資訊 ( $\\bf{f} \\odot \\ \\bf{c_{t-1}}$)\n",
    "\n",
    "**(2) 新的記憶單元: g**\n",
    "\n",
    "$$\\bf{g}=tanh \\left (\\bf{x_{t}}\\bf{W_{x}^{\\left ( g \\right )}}+ \\bf{h_{t-1}}\\bf{W_{h}^{\\left ( g \\right )}}+b^{\\left ( g \\right )} \\right )$$\n",
    "\n",
    "**說明:** 要在記憶單元($\\bf{c_{t}}$)增加的新記憶\n",
    "\n",
    "**(3) 輸入閘門(input gate): i**\n",
    "\n",
    "$$\\bf{i}=\\sigma \\left (\\bf{x_{t}}\\bf{W_{x}^{\\left ( i \\right )}}+ \\bf{h_{t-1}}\\bf{W_{h}^{\\left ( i \\right )}}+b^{\\left ( i \\right )} \\right )$$\n",
    "\n",
    "**說明:** 判斷新的記憶單元($\\bf{g}$)的各個元素是否有當作新資訊的價值，進而取捨要增加的資訊 ($\\bf{g}\\odot \\bf{i}$)\n",
    "\n",
    "**(4) 輸出閘門(output gate): o**\n",
    "\n",
    "$$\\bf{o}=\\sigma \\left (\\bf{x_{t}}\\bf{W_{x}^{\\left ( o \\right )}}+ \\bf{h_{t-1}}\\bf{W_{h}^{\\left ( o \\right )}}+b^{\\left ( o \\right )} \\right )$$\n",
    "\n",
    "$$\\bf{c_{t}}=f\\ \\odot \\ \\bf{c_{t-1}} + \\bf{g}\\  \\odot \\bf{i}$$\n",
    "\n",
    "$$\\bf{h_{t}}=\\bf{o}\\ \\odot \\ tanh\\left ( \\bf{c_{t}} \\right )$$\n",
    "\n",
    "**說明:** \n",
    " \n",
    "(i) tanh 函數的輸出可以解釋成某個編碼後資訊的強弱(程度) (作為活化函數使用)\n",
    "\n",
    "(ii) sigmoid 函數的輸出(0~1)代表的資料通過的比例\n",
    "\n",
    "(iii) 對 tanh$\\left ( \\bf{c_{t}} \\right )$ 套用輸出閘門 $\\bf{o}$ 可解釋為要用 tanh$\\left ( \\bf{c_{t}} \\right )$ 中的各個元素來調整下一個時刻每個隱藏狀態$\\left ( \\bf{h_{t}} \\right )$元素的重要程度\n",
    "\n",
    "#### 2. 單個 LSTM 的計算圖 (執行權重、偏權值的整合)\n",
    "\n",
    "<img src=\"img/affineLSTMcp.JPG\" alt=\"drawing\" width=\"400px\"/>\n",
    "\n",
    "$$\\bf{f}=\\sigma \\left (\\bf{x_{t}}\\bf{W_{x}^{\\left ( f \\right )}}+ \\bf{h_{t-1}}\\bf{W_{h}^{\\left ( f \\right )}}+b^{\\left ( f \\right )} \\right )$$\n",
    "\n",
    "$$\\bf{g}=tanh \\left (\\bf{x_{t}}\\bf{W_{x}^{\\left ( g \\right )}}+ \\bf{h_{t-1}}\\bf{W_{h}^{\\left ( g \\right )}}+b^{\\left ( g \\right )} \\right )$$\n",
    "\n",
    "$$\\bf{i}=\\sigma \\left (\\bf{x_{t}}\\bf{W_{x}^{\\left ( i \\right )}}+ \\bf{h_{t-1}}\\bf{W_{h}^{\\left ( i \\right )}}+b^{\\left ( i \\right )} \\right )$$\n",
    "\n",
    "$$\\bf{o}=\\sigma \\left (\\bf{x_{t}}\\bf{W_{x}^{\\left ( o \\right )}}+ \\bf{h_{t-1}}\\bf{W_{h}^{\\left ( o \\right )}}+b^{\\left ( o \\right )} \\right )$$\n",
    "\n",
    "$$\\bf{c_{t}}=f\\ \\odot \\ \\bf{c_{t-1}} + \\bf{g}\\  \\odot \\bf{i}$$\n",
    "\n",
    "$$\\bf{h_{t}}=\\bf{o}\\ \\odot \\ tanh\\left ( \\bf{c_{t}} \\right )$$\n",
    "\n",
    "**執行權重、偏權值的整合**\n",
    "\n",
    "當 $\\bf{W_{x}}=[\\bf{W_{x}^{\\left ( f \\right )}}, \\bf{W_{x}^{\\left ( g \\right )}}, \\bf{W_{x}^{\\left ( i \\right )}}, \\bf{W_{x}^{\\left ( o \\right )}}]$、$\\bf{W_{h}}=[\\bf{W_{h}^{\\left ( f \\right )}}, \\bf{W_{h}^{\\left ( g \\right )}},\\bf{W_{h}^{\\left ( i \\right )}}, \\bf{W_{h}^{\\left ( o \\right )}}]$、$\\bf{b}=[\\bf{b^{\\left ( f \\right )}}, \\bf{b^{\\left ( g \\right )}}, \\bf{b^{\\left ( i \\right )}}, \\bf{b^{\\left ( o \\right )}}]$時，\n",
    "\n",
    "仿射轉換(affine transformation)的統一處理為\n",
    "\n",
    "$$\\bf{x_{t}}\\bf{W_{x}} + \\bf{h_{t-1}}\\bf{W_{h}} + \\bf{b}$$\n",
    "\n",
    "#### 3. TimeLSTM 層的結構\n",
    "\n",
    "<img src=\"img/TimeLSTM.JPG\" alt=\"drawing\" width=\"600px\"/>\n",
    "\n",
    "$$\\bf{\\vec{x}_{s}}=\\left (x_{0}, x_{1}, x_{2},...,x_{T-1} \\right ),\\ \\bf{\\vec{h}_{s}}=\\left ( h_{0}, h_{1}, h_{2},...,h_{T-1} \\right )$$\n",
    "\n",
    "- Truncated BPTT 進行小批次學習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 建立 LSTM 層的類別:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "        \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        Wx, Wh, b = self.params    \n",
    "        N, H = h_prev.shape\n",
    "        \n",
    "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
    "        \n",
    "        # Slice\n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "        \n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "\n",
    "        c_next = f * c_prev + g * i\n",
    "        h_next = o * np.tanh(c_next)\n",
    "        \n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
    "        \n",
    "        return h_next, c_next\n",
    "    \n",
    "    def backward(self, dh_next, dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "\n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "\n",
    "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "\n",
    "        dc_prev = ds * f\n",
    "\n",
    "        di = ds * g\n",
    "        df = ds * c_prev\n",
    "        do = dh_next * tanh_c_next\n",
    "        dg = ds * i\n",
    "\n",
    "        di *= i * (1 - i)\n",
    "        df *= f * (1 - f)\n",
    "        do *= o * (1 - o)\n",
    "        dg *= (1 - g ** 2)\n",
    "\n",
    "        dA = np.hstack((df, dg, di, do))\n",
    "\n",
    "        dWh = np.dot(h_prev.T, dA)\n",
    "        dWx = np.dot(x.T, dA)\n",
    "        db = dA.sum(axis=0)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        dx = np.dot(dA, Wx.T)\n",
    "        dh_prev = np.dot(dA, Wh.T)\n",
    "\n",
    "        return dx, dh_prev, dc_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 建立 TimeLSTM 層的類別:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TimeLSTM:\n",
    "    def __init__(self, Wx, Wh, b, stateful = False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.c = None, None\n",
    "        self.dh = None\n",
    "        self.stateful = stateful\n",
    "    \n",
    "    def set_state(self, h, c = None):\n",
    "        self.h, self.c = h, c\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h, self.c = None, None\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        H = Wh.shape[0]\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        \n",
    "        if not self.stateful or self.c is None:\n",
    "            self.c = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = LSTM(*self.params)\n",
    "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
    "            hs[:, t, :] = self.h\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D = Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "        grads = [0, 0, 0]\n",
    "        \n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
    "            dxs[:, t, :] = dx\n",
    "            \n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        \n",
    "        self.dh = dh\n",
    "        \n",
    "        return dxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四、Gated Recurrent Unit (GRU)\n",
    "\n",
    "#### 1. 單個 GRU 的計算圖:\n",
    "\n",
    "<img src=\"img/GRU.JPG\" alt=\"drawing\" width=\"400px\"/>\n",
    "\n",
    "- 輸入值: $\\bf{x}$\n",
    "\n",
    "\n",
    "- 隱藏狀態(hidden state): $\\bf{h}$\n",
    "\n",
    "\n",
    "[註1] $\\sigma$ 代表 sigmoid 函數\n",
    "\n",
    "[註2] $\\odot$: 阿達馬乘積(hadamard product)，兩個矩陣相同位置的元素相乘\n",
    "\n",
    "**(1) 重置閘門: r**\n",
    "\n",
    "$$\\bf{r}=\\sigma \\left (\\bf{x_{t}}\\bf{W_{x}^{\\left ( r \\right )}}+ \\bf{h_{t-1}}\\bf{W_{h}^{\\left ( r \\right )}}+b^{\\left ( r \\right )} \\right )$$\n",
    "\n",
    "**說明:** 用來決定\"忽略\"過去隱藏狀態的程度\n",
    "\n",
    "**(2) 更新閘門: z**\n",
    "\n",
    "$$\\bf{z}=\\sigma \\left (\\bf{x_{t}}\\bf{W_{x}^{\\left ( z \\right )}}+\\bf{h_{t-1}}\\bf{W_{h}^{\\left ( z \\right )}}+b^{\\left ( z \\right )} \\right )$$\n",
    "\n",
    "$$\\bf{\\tilde{h}}=tanh\\left [\\bf{x_{t}}\\bf{W_{x}} + \\left (\\bf{r} \\odot \\bf{h_{t-1}} \\right )\\bf{W_{h}} + b \\right ]$$\n",
    "\n",
    "$$\\bf{h_{t}}=\\left (1-z \\right ) \\odot \\bf{h_{t-1}} + \\bf{z} \\odot \\bf{\\tilde{h}}$$\n",
    "\n",
    "**說明:** 用來更新隱藏狀態的閘門，相當於 LSTM 的遺忘閘門與輸入閘門。$\\left (1-\\bf{z} \\right ) \\odot \\bf{h_{t-1}}$ 代表刪除過去隱藏狀態所有必須要忘記的資料; $\\bf{z} \\odot \\bf{\\tilde{h}}$ 代表輸入閘門的權重加到新增的資料中。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
